\chapter{Introducción}
\label{cap:introduccion}

[Revisar]\\

La computación en la nube es un nuevo paradigma que pretende transformar la computación en un servicio. Durante estos últimos años la computación en la nube ha ido ganando importancia de manera progresiva, ya que la posibilidad de usar la computación como un servicio permite a los usuarios de una aplicación acceder a ésta a través de un navegador web, una aplicación móvil o un cliente de escritorio mientras que la lógica de la aplicación y los datos se encuentran en servidores situados en una localización remota. Esto brinda a las empresas la posibilidad de ofrecer servicios web sin tener que hacer una gran inversión inicial en infraestructura propia. Las aplicaciones en la nube tratan de proporcionar al usuario el mismo servicio y rendimiento que las aplicaciones instaladas localmente en su ordenador.\\

A lo largo de este proyecto se verán tres ejemplos de infraestructuras distribuidas. La primera de ellas es la infraestrucutra de ejecución de trabajos. Este tipo de infraestructuras está especializada en la ejecución de grandes cargas de trabajo paralelizable e intensivo en computación. Son por lo tanto idóneas para ser usadas en la computación de altas prestaciones. Dentro de esta infraestructura distribuida los ejemplos más claros que podemos encontrar son Condor y Torque.

La segunda infraestructura es la de servicios web en tres capas. Este tipo de infraestructuras tiene tres niveles claramente diferenciados: balanceo o distribución de carga, servidor web y base de datos. El primer nivel de esta arquitectura es el encargado de distribuir la carga (las peticiones web) a los elementos del segundo nivel. Éstos procesarán las peticiones web y para ello puede que tengan que consultar o modificar ciertos datos. Los datos de la aplicación se encuentran en el tercer nivel, y  por consiguiente, cada vez que uno de los elementos del segundo nivel necesite leer información de la base de datos o modificarla, accederá a este tercer nivel. En esta infraestructura no hay un ejemplo claro, pero cualquier página web profesional de hoy en día se sustenta en una arquitectura similar a ésta.

La tercera y última es AppScale, una implementación \emph{open source} del App Engine de Google. App Engine permite alojar aplicaciones web en la infraestructura que Google posee. Además de presentar esta funcionalidad AppScale también ofrece las APIs de EC2, MapReduce y Neptune. La API de EC2 añade a las aplicaciones la capacidad de interactuar con máquinas alojadas en Amazon EC2. La API de MapReduce permite escribir aplicaciones que hagan uso del \emph{framework} (o paradigma?) MapReduce. La última API, Neptune, añade a App Engine la capacidad de usar los nodos de la infraestructura para ejecutar trabajos. Los trabajos más representativos que puede ejecutar son: de entrada, de salida y MPI. El trabajo de entrada sirve para subir ficheros (generalmente el código que se ejecutará) a la infraestructura, el de salida para traer ficheros (generalmente los resultados obtenidos después de la ejecución) y el de MPI para ejecutar un trabajo MPI.

La infraestructura necesaria para dar soporte a todas estas APIs ya no es tan sencilla como en los dos casos anteriores. De hecho, las anteriores infraestructuras estarían contenidas en ésta. Hay dos maneras de definir la infraestructura de AppScale. En la primera de ellas se define un despliegue por defecto, en el que un nodo toma el rol de \emph{controller} y el resto de nodos toman el rol de \emph{servers}. El nodo \emph{controller} es el que carga con la responsabilidad de la coordinación y los nodos \emph{servers} son los que llevan cabo la mayor parte del trabajo. La segunda manera de definir la infraestructura es hacerlo a través de un despliegue personalizado. En este despliegue podemos definir con más precisión los roles que queremos que desempeñe un nodo. Entre todos los posibles roles que AppScale ofrece, los más interesantes desde nuestro punto de vista son: \emph{master}, \emph{appengine}, \emph{database}, \emph{login} y \emph{open}.\\

De igual manera, las herramientas de administración de sistemas (o herramientas de gestión de configuración) también han experimentado un considerable avance. Con entornos cada vez más heterogéneos y complejos la administración de sistemas de manera manual ya no es una opción. Entre todo el conjunto de  herramientas destacan de manera especial Puppet y CFEngine. Puppet es una herramienta de gestión de configuración basada en un lenguaje  declarativo: el usuario especifica qué estado debe alcanzarse y Puppet se encarga de hacerlo. CFEngine permite al usuario un control más fino de cómo se hacen las cosas, mejorando el rendimiento a costa de perder abstracciones de más alto nivel.\\

Sin embargo, estas herramientas de gestión de la configuración carecen de la funcionalidad requerida para administrar infraestructuras distribuidas.\\

Si tomamos la administración de un cloud como la administración de las MV que forman los nodos del cloud nos damos cuenta de que la administración es puramente software.\\


\section{Contexto del proyecto}

Para la realización de este proyecto de fin de carrera se ha hecho uso del laboratorio 1.03b de investigación que el Departamento de Informática e Ingeniería de Sistemas posee en la Escuela de Ingeniería y Arquitectura de la Universidad de Zaragoza. Los ordenadores que forman este laboratorio poseen procesadores con soporte de virtualización, lo que permite la creación de diversas máquinas virtuales. La creación de los distintos tipos de cloud que representan cada una de las infraestructuras distribuidas se ha llevado a cabo a través de máquinas virtuales alojadas en distintos ordenadores del laboratorio.\\

En este laboratorio se ha comprobado la validez de la extensión para administración de clouds introducida en la herramienta de gestión de configuraciones Puppet que se ha desarrollado a lo largo del proyecto de fin de carrera.


\section{Objetivos}

El objetivo de este proyecto es proporcionar una herramienta que facilite la puesta en marcha de infraestructuras distribuidas y su posterior mantenimiento. Las tareas principales en las que se puede dividir este proyecto son:

\begin{enumerate}
\item Estudio de algunas de las infraestructuras distribuidas existentes profundizando en la parte relativa a la ejecución de trabajos distribuidos.
\item Análisis de las herramientas de administración de virtualización \emph{hardware}.
\item Investigación de las herramientas de gestión de configuración existentes más relevantes y elección de aquella que mayor facilidad de integración y uso proporcione.
\item Extensión de la herramienta de gestión de configuración para que soporte la puesta en marcha y el mantenimiento de un sistema de ejecución de trabajos distribuidos.
\end{enumerate}


\section{Tecnología utilizada}

Para la elaboración de este proyecto se ha hecho uso de las siguientes tecnologías:
\begin{itemize}
\item KVM, QEMU, libvirt y virsh para el soporte de virtualización.
\item Ruby como lenguaje de programación para la extensión de Puppet.
\item Shell como lenguaje de programación de los \emph{scripts} de configuración.
\item Sistema operativo Debian para las máquinas del laboratorio y Ubuntu para las máquinas virtuales.
\item \LaTeX{} \cite{manual:latex} para la redacción de esta memoria.
\end{itemize}


\section{Organización de la memoria}

El resto de este documento queda organizado de la siguiente manera:
\begin{description}
\item[Capítulo \ref{cap:metodologia}] Metodología seguida durante la realización del proyecto.
\item[Capítulo \ref{cap:trabajo}] Extensión de Puppet para soporte de infraestructuras distribuidas.
\item[Capítulo \ref{cap:appscale}] Uso de la extensión de Puppet para soporte de infraestructura AppScale.
\item[Capítulo \ref{cap:web}] Uso de la extensión de Puppet para soporte de infraestructura web de tres niveles.
\item[Capítulo \ref{cap:torque}] Uso de la extensión de Puppet para soporte de infraestructura de trabajos distribuidos.
\item[Capítulo \ref{cap:conclusiones}] Conclusiones.
\end{description}


\section{Agradecimientos}

Agradecimientos
